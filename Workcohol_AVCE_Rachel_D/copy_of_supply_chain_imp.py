# -*- coding: utf-8 -*-
"""Copy of supply chain imp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rz3ylw1sQVZU8edGO-5e8vjK2jY9hqOM
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error

df=pd.read_csv('/content/supply_chain_up-to-date.csv')
df

df.head(10)

#Inventory Optimization (Predict Stock Levels) ðŸ“¦
#Objective: Predict stock levels to ensure optimal inventory and prevent over/under-stocking.
#Target Variable: Stock levels
#Key Features:
#Number of products sold, Revenue generated, Order quantities, Lead times, Supplier name, Manufacturing lead time, Production volumes

'''Feature Selection (Key Predictors for Stock Levels)
Demand-related: Number of products sold, Revenue generated, Order quantities
Supply-related: Lead times, Supplier name, Manufacturing lead time, Production volumes
Logistics-related: Shipping times, Shipping carriers, Shipping costs, Transportation modes, Routes
Product-related: Product type, Price, Availability'''

goal=['stocklevels']

feature_selection=['Number of products sold', 'Revenue generated', 'Order quantities',
                   'Lead times, Supplier name', 'Manufacturing lead time', 'Production volumes',
                   'Shipping times', 'Shipping carriers', 'Shipping costs', 'Transportation modes', 'Routes',
                   'Product type', 'Price', 'Availability'

]

df.info()

df.describe()

df.isna().sum()

# Drop Unnecessary Columns
df.drop(columns=["SKU"], inplace=True)

df.drop(columns=['Inspection results'], inplace=True)

unique_counts = df.nunique()
print(unique_counts)

categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
print("Categorical Columns:", categorical_cols)

# Encode Categorical Variables
categorical_cols = ["Product type", "Supplier name", "Location", "Shipping carriers", "Transportation modes" ,  "Routes", "Product type", "Availability"]
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le  # Store encoders for inverse transformation if needed

df

# Initialize the LabelEncoder
le_routes = LabelEncoder()
# Apply Label Encoding to 'Route' column
df["Routes"] = le_routes.fit_transform(df["Routes"])

df

"""**Feature** **Engineering**"""

# Create Lag Features (Previous stock levels)
df['Stock_Level_t-1'] = df['Stock levels'].shift(1)
df['Stock_Level_t-2'] = df['Stock levels'].shift(2)

# Check for missing values generated due to shift (drop if needed)
df.dropna(subset=['Stock_Level_t-1', 'Stock_Level_t-2'], inplace=True)

print(df[['Stock levels', 'Stock_Level_t-1', 'Stock_Level_t-2']].head())

# Calculate Rolling Averages of Stock Levels (3-day, 7-day windows)
df['Stock_Level_MA_3'] = df['Stock levels'].rolling(window=3).mean()
df['Stock_Level_MA_7'] = df['Stock levels'].rolling(window=7).mean()

# Check for missing values due to rolling window (drop if needed)
df.dropna(subset=['Stock_Level_MA_3', 'Stock_Level_MA_7'], inplace=True)

print(df[['Stock levels', 'Stock_Level_MA_3', 'Stock_Level_MA_7']].head())

# Assume Safety Stock is a constant value (can be customized based on your data)
safety_stock = 100  # Example, can be adjusted

# Calculate Lead Time Demand: Average daily demand * Lead time (Assumed here)
df['Lead_Time_Demand'] = df['Lead times'] * df['Number of products sold'].mean() / 30  # Example, for a monthly demand

# Reorder Point Calculation: Lead Time Demand + Safety Stock
df['Reorder_Point'] = df['Lead_Time_Demand'] + safety_stock

print(df[['Lead_Time_Demand', 'Reorder_Point']].head())

# Stock Turnover Ratio = Revenue generated / Average Stock Level
df['Avg_Stock_Level'] = (df['Stock levels'] + df['Stock_Level_t-1']) / 2  # Calculate average stock level
df['Stock_Turnover_Ratio'] = df['Revenue generated'] / df['Avg_Stock_Level']

print(df[['Revenue generated', 'Avg_Stock_Level', 'Stock_Turnover_Ratio']].head())

# DIO = (Average Stock Level / Cost of Goods Sold per day) * 365
# Here, assuming an average COGS per day can be derived from total revenue (replace with actual if available)

df['COGS_per_day'] = df['Revenue generated'] / 30  # Example, assuming monthly revenue
df['DIO'] = (df['Avg_Stock_Level'] / df['COGS_per_day']) * 30  # Monthly DIO (adjust as needed)

print(df[['Avg_Stock_Level', 'COGS_per_day', 'DIO']].head())

df.head(5)

df['Product type']



pt=df['Product type'].iloc[0]
print("Product type at Index 0:", pt)

# Create a label encoder
le = LabelEncoder()

# Assuming 'Gender' is the column you want to encode
df['Customer demographics'] = le.fit_transform(df['Customer demographics'])

df['Customer demographics']

feature_selection =['Product type', 'Price', 'Availability', 'Number of products sold', 'Revenue generated', 'Customer demographics', 'Lead times', 'Order quantities',
                    'Shipping times', 'Shipping carriers', 'Shipping costs', 'Supplier name', 'Location', 'Lead time', 'Production volumes', 'Manufacturing lead time', 'Manufacturing costs',
                    'Defect rates', 'Transportation modes', 'Routes', 'Costs', 'Stock_Level_t-1', 'Stock_Level_t-2', 'Stock_Level_MA_3', 'Stock_Level_MA_7', 'Lead_Time_Demand', 'Reorder_Point',
                    'Avg_Stock_Level', 'Stock_Turnover_Ratio', 'COGS_per_day', 'DIO']

supplier_name_at_index_0 = df['Supplier name'].iloc[0]
print("Supplier Name at Index 0:", supplier_name_at_index_0)

carriers=df['Shipping carriers'].iloc[0]
print("Shipping carriers at Index 0:", carriers)

tr=df['Transportation modes'].iloc[0]
print("Transportation modes at Index 0:", tr)

! pip install --upgrade scikit-learn

! pip install --upgrade xgboost

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# Load and preprocess the data (assuming df is your dataset)
# Example: df contains columns: 'sales', 'demand', 'lead_time', 'seasonality', 'stock_levels'

# Data Preprocessing
X = df.drop(columns=['Stock levels'])  # Features (e.g., 'sales', 'demand', etc.)
y = df['Stock levels']  # Target variable

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (if necessary)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Predict stock levels
y_pred = model.predict(X_test_scaled)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print results
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"Mean Absolute Error: {mae}")
print(f"R-squared (RÂ²): {r2}")

# Further improvements could include hyperparameter tuning, feature engineering, etc.

from sklearn.model_selection import train_test_split

# Assuming your dataset is in X (features) and y (target)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% for training, 20% for temp
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 50% of 20% for validation, 50% for testing

import xgboost
print(xgboost.__version__)

from xgboost import XGBRegressor
model = XGBRegressor(random_state=42)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Calculate evaluation metrics
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Print the results
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"Mean Absolute Error: {mae}")
print(f"R-squared (RÂ²): {r2}")

import matplotlib.pyplot as plt

# Get feature importance
importance = model.feature_importances_

# Plot feature importance
plt.barh(range(len(importance)), importance)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance for XGBRegressor')
plt.show()

residuals = y_test - predictions
plt.scatter(predictions, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

import pickle

# Assuming 'model' is your trained machine learning model
with open("model.pkl", "wb") as file:
    pickle.dump(model, file)

print("Model saved successfully!")

print("Training Data Columns: ", X_train.columns)
print("Test Data Columns: ", X_test.columns)
print("Number of Features in Train Data: ", X_train.shape[1])
print("Number of Features in Test Data: ", X_test.shape[1])

df.iloc[0]

print("Model was trained with features:", list(X_train.columns))
print("Total features:", len(X_train.columns))